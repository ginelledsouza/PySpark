{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "Spark = SparkSession.builder.appName(\"Demonstration\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Demonstration</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1fc48aed280>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records: 5110\n",
      "Total Columns: 12\n"
     ]
    }
   ],
   "source": [
    "df = Spark.read.csv(\"Dataset/stroke.csv\",header=True,inferSchema=True)\n",
    "print(\"Total Records: {}\\nTotal Columns: {}\".format(df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "|   id|gender| age|hypertension|heart_disease|ever_married|    work_type|Residence_type|avg_glucose_level| bmi| smoking_status|stroke|\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "| 9046|  Male|67.0|           0|            1|         Yes|      Private|         Urban|           228.69|36.6|formerly smoked|     1|\n",
      "|51676|Female|61.0|           0|            0|         Yes|Self-employed|         Rural|           202.21| N/A|   never smoked|     1|\n",
      "|31112|  Male|80.0|           0|            1|         Yes|      Private|         Rural|           105.92|32.5|   never smoked|     1|\n",
      "|60182|Female|49.0|           0|            0|         Yes|      Private|         Urban|           171.23|34.4|         smokes|     1|\n",
      "| 1665|Female|79.0|           1|            0|         Yes|Self-employed|         Rural|           174.12|  24|   never smoked|     1|\n",
      "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 5 records within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------------+-------------+------------+---------+--------------+-----------------+---+--------------+------+\n",
      "| id|gender|age|hypertension|heart_disease|ever_married|work_type|Residence_type|avg_glucose_level|bmi|smoking_status|stroke|\n",
      "+---+------+---+------------+-------------+------------+---------+--------------+-----------------+---+--------------+------+\n",
      "|  0|     0|  0|           0|            0|           0|        0|             0|                0|  0|             0|     0|\n",
      "+---+------+---+------------+-------------+------------+---------+--------------+-----------------+---+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are no null/none values present within the dataset. However, while inspecting the data we observed that the column \"BMI\" contains the value \"N/A\" that refers to null/none values. We will deal with this and replace it with a null character so that we can fill these values with the mean of the BMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- hypertension: integer (nullable = true)\n",
      " |-- heart_disease: integer (nullable = true)\n",
      " |-- ever_married: string (nullable = true)\n",
      " |-- work_type: string (nullable = true)\n",
      " |-- Residence_type: string (nullable = true)\n",
      " |-- avg_glucose_level: double (nullable = true)\n",
      " |-- bmi: integer (nullable = true)\n",
      " |-- smoking_status: string (nullable = true)\n",
      " |-- stroke: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "value = str(round(df.agg({'bmi':'mean'}).collect()[0][0],2))\n",
    "df = df.na.replace('N/A', value,'bmi')\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "df = df.withColumn(\"bmi\", df[\"bmi\"].cast(IntegerType()))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "\n",
      "gender\n",
      "['Female', 'Other', 'Male']\n",
      "\n",
      "**********\n",
      "\n",
      "ever_married\n",
      "['No', 'Yes']\n",
      "\n",
      "**********\n",
      "\n",
      "work_type\n",
      "['Never_worked', 'Self-employed', 'Private', 'children', 'Govt_job']\n",
      "\n",
      "**********\n",
      "\n",
      "Residence_type\n",
      "['Urban', 'Rural']\n",
      "\n",
      "**********\n",
      "\n",
      "smoking_status\n",
      "['smokes', 'Unknown', 'never smoked', 'formerly smoked']\n",
      "\n",
      "**********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [x[0] for x in df.dtypes if (x[1] == 'string')]\n",
    "\n",
    "print(\"**********\\n\")\n",
    "\n",
    "for x in columns:\n",
    "    print(x)\n",
    "    print([i[x] for i in df.select(x).distinct().collect()])\n",
    "    print(\"\\n**********\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns \"Gender and Smoking Status\" have two values that are \"Other and Unknown\". We can replace these values with the most frequent value of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = [df.groupby(\"gender\").count().orderBy(\"count\", ascending=False).first()[0]][0]\n",
    "df = df.na.replace('Other', value,'gender')\n",
    "\n",
    "value = [df.groupby(\"smoking_status\").count().orderBy(\"count\", ascending=False).first()[0]][0]\n",
    "df = df.na.replace('Unknown', value,'smoking_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------+\n",
      "|summary|               age| avg_glucose_level|              bmi|\n",
      "+-------+------------------+------------------+-----------------+\n",
      "|  count|              5110|              5110|             5110|\n",
      "|   mean|43.226614481409015|106.14767710371804|28.43091976516634|\n",
      "| stddev| 22.61264672311348| 45.28356015058193|7.688482598387539|\n",
      "|    min|              0.08|             55.12|               10|\n",
      "|    max|              82.0|            271.74|               97|\n",
      "+-------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['summary'] + [x[0] for x in df.dtypes if (x[1] != 'string') and (len(df.select(x[0]).distinct().collect()) != 2)]\n",
    "columns.remove('id')\n",
    "\n",
    "data_summary = df.describe().select(columns)\n",
    "\n",
    "data_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statistical summary of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender\n",
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|  Male| 2115|\n",
      "|Female| 2995|\n",
      "+------+-----+\n",
      "\n",
      "\n",
      "ever_married\n",
      "+------------+-----+\n",
      "|ever_married|count|\n",
      "+------------+-----+\n",
      "|          No| 1757|\n",
      "|         Yes| 3353|\n",
      "+------------+-----+\n",
      "\n",
      "\n",
      "work_type\n",
      "+-------------+-----+\n",
      "|    work_type|count|\n",
      "+-------------+-----+\n",
      "| Never_worked|   22|\n",
      "|     Govt_job|  657|\n",
      "|     children|  687|\n",
      "|Self-employed|  819|\n",
      "|      Private| 2925|\n",
      "+-------------+-----+\n",
      "\n",
      "\n",
      "Residence_type\n",
      "+--------------+-----+\n",
      "|Residence_type|count|\n",
      "+--------------+-----+\n",
      "|         Rural| 2514|\n",
      "|         Urban| 2596|\n",
      "+--------------+-----+\n",
      "\n",
      "\n",
      "smoking_status\n",
      "+---------------+-----+\n",
      "| smoking_status|count|\n",
      "+---------------+-----+\n",
      "|         smokes|  789|\n",
      "|formerly smoked|  885|\n",
      "|   never smoked| 3436|\n",
      "+---------------+-----+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [x[0] for x in df.dtypes if (x[1] == 'string')]\n",
    "\n",
    "for i in columns:\n",
    "    \n",
    "    print(i)\n",
    "    df.groupBy(i).count().orderBy('count').show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that in the current dataset, a majority of records are from female patients. It also consists of many records from patients that are married as well as patients who have never smoked. On close observation at the working type of the patients, many work in private organizations or are self-employed. There is no difference in the type of residence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient Stroke vs Gender Analysis\n",
      "+------+----+---+\n",
      "|gender|   0|  1|\n",
      "+------+----+---+\n",
      "|Female|2854|141|\n",
      "|  Male|2007|108|\n",
      "+------+----+---+\n",
      "\n",
      "Patient Stroke vs Ever Married Analysis\n",
      "+------------+----+---+\n",
      "|ever_married|   0|  1|\n",
      "+------------+----+---+\n",
      "|          No|1728| 29|\n",
      "|         Yes|3133|220|\n",
      "+------------+----+---+\n",
      "\n",
      "Patient Stroke vs Work Type Analysis\n",
      "+-------------+----+----+\n",
      "|    work_type|   0|   1|\n",
      "+-------------+----+----+\n",
      "| Never_worked|  22|null|\n",
      "|Self-employed| 754|  65|\n",
      "|      Private|2776| 149|\n",
      "|     children| 685|   2|\n",
      "|     Govt_job| 624|  33|\n",
      "+-------------+----+----+\n",
      "\n",
      "Patient Stroke vs Residence Type Analysis\n",
      "+--------------+----+---+\n",
      "|Residence_type|   0|  1|\n",
      "+--------------+----+---+\n",
      "|         Urban|2461|135|\n",
      "|         Rural|2400|114|\n",
      "+--------------+----+---+\n",
      "\n",
      "Patient Stroke vs Smoking Status Analysis\n",
      "+---------------+----+---+\n",
      "| smoking_status|   0|  1|\n",
      "+---------------+----+---+\n",
      "|         smokes| 747| 42|\n",
      "|   never smoked|3299|137|\n",
      "|formerly smoked| 815| 70|\n",
      "+---------------+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [x[0] for x in df.dtypes if (x[1] == 'string')]\n",
    "\n",
    "for i in columns:\n",
    "    print(\"Patient Stroke vs {} Analysis\".format(i.title().replace(\"_\",\" \")))\n",
    "    df.groupBy(i).pivot(\"stroke\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have identified all the absolute values for patients suffering and not suffering from stroke for various factors. We will now analyze the data in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient Stroke vs Gender Detailed Analysis\n",
      "+------+----+---+-----------------+\n",
      "|gender|   0|  1|stroke_precentage|\n",
      "+------+----+---+-----------------+\n",
      "|Female|2854|141|                4|\n",
      "|  Male|2007|108|                5|\n",
      "+------+----+---+-----------------+\n",
      "\n",
      "Patient Stroke vs Ever Married Detailed Analysis\n",
      "+------------+----+---+-----------------+\n",
      "|ever_married|   0|  1|stroke_precentage|\n",
      "+------------+----+---+-----------------+\n",
      "|          No|1728| 29|                1|\n",
      "|         Yes|3133|220|                6|\n",
      "+------------+----+---+-----------------+\n",
      "\n",
      "Patient Stroke vs Work Type Detailed Analysis\n",
      "+-------------+----+----+-----------------+\n",
      "|    work_type|   0|   1|stroke_precentage|\n",
      "+-------------+----+----+-----------------+\n",
      "| Never_worked|  22|null|             null|\n",
      "|Self-employed| 754|  65|                7|\n",
      "|      Private|2776| 149|                5|\n",
      "|     children| 685|   2|                0|\n",
      "|     Govt_job| 624|  33|                5|\n",
      "+-------------+----+----+-----------------+\n",
      "\n",
      "Patient Stroke vs Residence Type Detailed Analysis\n",
      "+--------------+----+---+-----------------+\n",
      "|Residence_type|   0|  1|stroke_precentage|\n",
      "+--------------+----+---+-----------------+\n",
      "|         Urban|2461|135|                5|\n",
      "|         Rural|2400|114|                4|\n",
      "+--------------+----+---+-----------------+\n",
      "\n",
      "Patient Stroke vs Smoking Status Detailed Analysis\n",
      "+---------------+----+---+-----------------+\n",
      "| smoking_status|   0|  1|stroke_precentage|\n",
      "+---------------+----+---+-----------------+\n",
      "|         smokes| 747| 42|                5|\n",
      "|   never smoked|3299|137|                3|\n",
      "|formerly smoked| 815| 70|                7|\n",
      "+---------------+----+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [x[0] for x in df.dtypes if (x[1] == 'string')]\n",
    "\n",
    "for i in columns:\n",
    "    print(\"Patient Stroke vs {} Detailed Analysis\".format(i.title().replace(\"_\",\" \")))\n",
    "    Data = df.groupBy(i).pivot(\"stroke\").count()\n",
    "    Data = Data.withColumn(\"stroke_precentage\",(Data[\"1\"]/(Data[\"0\"]+Data[\"1\"]))*100)\n",
    "    Data = Data.withColumn(\"stroke_precentage\", Data[\"stroke_precentage\"].cast(IntegerType()))\n",
    "    Data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Married men are more likely to suffer a stroke. It is observed that self-employed patients are seven times more likely to suffer a stroke, whereas patients with a private or government job are five times likely to suffer a stroke. We notice that patients in the Urban area suffer a stroke a little more than those from the rural areas. Lastly, the patients who have formerly been smoking have much higher chances of having a stroke than those who have never smoked.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling Using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler=VectorAssembler(inputCols=[\"age\",\"Experience\"],outputCol=\"Independent Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------------+------------------+------------------+\n",
      "|gender_OHE|ever_married_OHE|work_type_OHE|Residence_type_OHE|smoking_status_OHE|\n",
      "+----------+----------------+-------------+------------------+------------------+\n",
      "|       1.0|             0.0|          0.0|               0.0|               1.0|\n",
      "|       0.0|             0.0|          1.0|               1.0|               0.0|\n",
      "|       1.0|             0.0|          0.0|               1.0|               0.0|\n",
      "|       0.0|             0.0|          0.0|               0.0|               2.0|\n",
      "|       0.0|             0.0|          1.0|               1.0|               0.0|\n",
      "+----------+----------------+-------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "columns = [x[0] for x in df.dtypes if (x[1] == 'string')]\n",
    "columnsrename = [\"{}_OHE\".format(i) for i in columns]\n",
    "\n",
    "indexer=StringIndexer(inputCols=columns,outputCols=columnsrename)\n",
    "Data=indexer.fit(df).transform(df)\n",
    "Data.select(columnsrename).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------+\n",
      "|Independent Attributes|stroke|\n",
      "+----------------------+------+\n",
      "|  [67.0,0.0,1.0,228...|     1|\n",
      "|  (10,[0,3,4,7,8],[...|     1|\n",
      "|  [80.0,0.0,1.0,105...|     1|\n",
      "|  (10,[0,3,4,9],[49...|     1|\n",
      "|  [79.0,1.0,0.0,174...|     1|\n",
      "+----------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "columns = [x[0] for x in df.dtypes if (x[1] != 'string')]\n",
    "columns.remove('id')\n",
    "columns.remove('stroke')\n",
    "\n",
    "columns = columns + columnsrename\n",
    "\n",
    "featureassembler = VectorAssembler(inputCols=columns,outputCol=\"Independent Attributes\")\n",
    "\n",
    "OutputData = featureassembler.transform(Data)\n",
    "\n",
    "OutputData.select([\"Independent Attributes\",\"stroke\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingData = OutputData.select([\"stroke\",\"Independent Attributes\"])\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "training_data, testing_data = TrainingData.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------+--------------------+--------------------+----------+-------------+\n",
      "|stroke|Independent Attributes|       rawPrediction|         probability|prediction|       leafId|\n",
      "+------+----------------------+--------------------+--------------------+----------+-------------+\n",
      "|     0|  (10,[0,1,3,4],[36...|[2.83906906318637...|[0.94635635439545...|       0.0|[0.0,0.0,0.0]|\n",
      "|     0|  (10,[0,1,3,4],[40...|[2.83906906318637...|[0.94635635439545...|       0.0|[0.0,0.0,0.0]|\n",
      "|     0|  (10,[0,1,3,4],[51...|[2.83906906318637...|[0.94635635439545...|       0.0|[0.0,0.0,0.0]|\n",
      "|     0|  (10,[0,1,3,4],[55...|[2.83906906318637...|[0.94635635439545...|       0.0|[0.0,0.0,0.0]|\n",
      "|     0|  (10,[0,1,3,4],[57...|[2.83906906318637...|[0.94635635439545...|       0.0|[0.0,0.0,0.0]|\n",
      "+------+----------------------+--------------------+--------------------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "model = RandomForestClassifier(numTrees=3, maxDepth=2, featuresCol='Independent Attributes', labelCol=\"stroke\", seed=42,leafCol=\"leafId\")\n",
    "model = model.fit(training_data)\n",
    "\n",
    "prediction = model.transform(testing_data)\n",
    "\n",
    "prediction.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeleval = MulticlassClassificationEvaluator(labelCol=\"stroke\", predictionCol=\"prediction\",metricName = \"accuracy\")\n",
    "modelaccuracy = modeleval.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9607993850883936\n",
      "Test Error: 0.039200614911606424\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format(modelaccuracy))\n",
    "print(\"Test Error: {}\".format(1.0-modelaccuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
